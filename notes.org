#+TITLE: Discrete Geometry 1
#+AUTHOR: Hari
#+LATEX_HEADER: \usepackage[left=2cm, right=2cm, bottom=2cm, top=2cm]{geometry}
#+LATEX_HEADER: \usepackage{parskip}
#+LATEX_HEADER: \usepackage{mathrsfs}
#+LATEX_HEADER: \def\R{\mathbb{R}}
#+LATEX_HEADER: \def\Z{\mathbb{Z}}
#+LATEX_HEADER: \def\inte{\operatorname{int}}
#+LATEX_HEADER: \def\pos{\operatorname{pos}}
#+LATEX_HEADER: \def\relint{\operatorname{rel\ int}}
#+LATEX_HEADER: \def\conv{\operatorname{Conv}}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
* Lecture 2 <2018-10-17 Wed>

** Definitions
*** Positive half
*** Affine hyperplane
*** (Question) What is the space of all oriented hyperplanes of $\R^n$
*** Definition of pos?
** Radon's theorem
   Three cases

   - If the size of M is greater than or equal to n+2, there is a radon partiiton
   - If the size of $M$ is greater than or equal to $n+1$, $0$ is an appex of
     $M$ and $0\neq M$. or $|M| \ge n+2$.

     then there is a partition of $M_1$ and $M_2$ of $M$ such that $\pos M_1
     \cap pos M_2 \neq \emptyset$.
   - partition is unique if and only i f
     - $|M| = n+2$ and $n+1$ points of $m$ are affinely dependent.
     - $|M| = n+1$ and no $n$ vectors of $M$ are linearly independent.
** Definitions
*** Radon partition
    A partition $M_1, M_2$ such that $\conv M_1 \cap conv M_2 \neq \emptyset$ is called a *Radon partition*.
** Proof
*** Part 1
    Size of $M$ greater than $n+2$. Take $x_1, \cdots, x_{n+2}$ pairwise disjoint points from $M.

    Then $x_1, \cdots x_{n+2}$ are affinely dependent, meaning that that you can
    choose scalars, real such that $\sum \lambda_i x_i = 0$.

    We can assume that for some $1 \le j \le n+1$ holds

    $\lambda_1 >0, \cdots, \lambda_i >0$ and $\lambda_{i+1} \le 0, \cdots, \lambda{n+2} \le 0$.

    $\lambda = \lambda_1 + \cdots + \lambda_j = -(\lambda_{i+1} + \cdots + \lambda_{n+2}$.

    $X=\frac1\lambda (\lambda_1 x_1 + \cdots \lambda_j x_j)$ a convex
    combination $\in \conv\{x_1, \cdots, x_j\}$.

    $$\frac{1}{\lambda}(\lambda_1x_1 + \cdots + \lambda_{n+2}x_{n+2}) = 0$$


    $$X= \frac{1}{\lambda}(\lambda_1x_1 + \cdots + \lambda_{j}x_{j})  = -\frac{1}{\lambda}(\lambda_jx_j + \cdots + \lambda_{n+2}x_{n+2}) = 0$$

    a convex combination.

    Thus $X$ is inside both the intersection.
*** Part 2

    $\pos M$ is a cone with appex ... 

    *A drawing*

    For every point $x_i \in M$, there is $\alpha_i > 0$, such that $\alpha_ix_i
    \in H'$,,

    blah blah.

    Keyword: matroids.
*** Part 3
    I didn't type.

    (A part.) A partition is unique.

    Let $\vert M \vert = n+2$ and $x_1, \cdots, x_{n+2}$ are affinely dependent implies
    $x_1, \cdots x_{n+1}$ (dropped one point) have to be on a affine hyperplane.
    Then by Radon's theorem, there is a partition $M_1'$ and $M_2'$ such that
    the convex hulls of these intersect. But then we can add the element
    $x_{n+2}$ to $M_1'$ or $M_2'$ to form two different partitions. Thus we have
    two Radon partitions of $M$ and thus this is a contradiction.

    Case for $|M| \ge n+3$. Let $\tilde{M} \subset M$ and $|\tilde{M}| = n+2$
    such that $M-\tilde M \neq \emptyset$. Then $\tilde M_1, \tilde M_2$ is a
    partition of $\tilde M$.

    $\conv \tilde M_1 \cap \conv \tilde M_2 \neq \emptyset$.

    $$\tilde M_1 \cap (M-\tilde M), \tilde M_2$$

    and $$\tilde M_2 \cap (M-\tilde M)$$ forms two different radon partitions and
    again, we have a contradiction.
** Questions 
*** About affine maps
    We have an arbitrary affine map between a simplex (n+1 dimensional) and
    $\R^n$. implies, there exist faces of the simpleces such that the faces do
    not intersect, but the images of the faces will intersect. Apparently this
    follows from the Radon's theorem. A different formulation of Radon's theorem

    $a\colon T_{n+1} \rightarrow \R^n$

    *Question*: Replace the affine map by a continuous map and is it still true? [fn:1]

    *Question*: How many points in $M \subset \R^d$ you should have to generate
    that for $n\ge 2$, there is a partition $M_1, \cdots, M_r$ of $M$ such that
    the intersection of the convex hulls of $M_i$ are non-empty. 

    *Question*: More points, minimal number of points?
* Lecture 2 <2018-10-23 Tue>
** Review
*** Radon's theorem
    1. If $M \subset \R^n$ and $\vert M \vert \ge n+2$, then there exists a
       partition $M_1$ and $M_2$ of $M$ such that $\conv M_1 \cap \conv M_2 \neq
       \emptyset$.
    2. If $M \subset \R^n$ and either $\vert M \vert \ge n+2$ and $0\neq M$ on
       $\vert M \vert \ge n+2$, then there is a partition of $M_1$, $M_2$ of $M$
       such that $\pos M_1 \cap \pos M_2 \neq \empty$
** Charatheodery's theorem
   1. Let $M\subset \R^n$, then $\conv M$ is the set of all convex combinations
      of at most $n+1$ points from $M$. [fn:2]
   2. Let $M\subset \R^n$. Then $\pos M$ is the set of all positive combinations
      of at most $n$ points from $M$.
*** Proof
    $x\in \conv M \implies$ there exists $\lambda_1, \cdots, \lambda_n$ and $x_1,\cdots, x_n \in M$. 

    $x=\lambda_1x_1+\cdots + \lambda_rx_r$ and $\sum \lambda_i = 1$, $\lambda_i \ge 0$.
    
    Let the presentation be such that $r$ is minimal. (We can do this because we
    are taking minimum over natural numbers.) Let us assume that $M \ge n+2$,
    then there exists an affine dependence $\mu_1x_2 + \cdots \mu_rx_r = 0$,
    $\mu_1 + \cdots + \mu_r = 0$ and not all $\mu_i$ 's are zero.
    
    (Basically the idea is that we assume the minimality of $r$ and if $r \ge
    n+2$, then there is an affine dependence, and then use this to contradict
    the  minimality of $r$.)[fn:3]
*** Lemma about compactness of convex hull of compact set
    $M^{n+1} \times \Delta \rightarrow M$. Here the space on the left is the set
    of all $n+1$ points of $M$ and $\Delta$ is a simplex.

    Now, it follows from the fact that image of a compact set is compact.
** Nearest points map and supporting hyperplane
*** Lemma
    Let $K\subset \R^n$ be closed and convex. Then for every $x\in \R^n$, there
    is unique point $x^1 \in K$ such that $$\Vert x - x^1\Vert = \inf\Vert x -
    y\Vert = d(x, K)$$
*** Proof
    We can find a sequence of points $(y_n)$ in $M$ such that the distance from
    $x$ is less than $1/n$. Now, the sequence is Cauchy. Since, $\R^n$ is
    complete, it has to converge, and since $K$ is closed, we are done. [fn:4]

    Uniqueness: Given $X$, if there are two points $x'$ and $x''$ such that the
    distances from $x$ from these two points are the same. In the plane $x, x',
    x''$, the triangle $\Delta x x' x''$ exists. But then a perpendicular to the
    side $x'x''$ would be smaller than the distance to $x'$ or $x''$. This is a
    contradiction. Hence the points have to be unique. (Here the convexity of
    the set is used.)
*** Definition of nearest point map
    Given $K\subset \R^n$ be a closed convex set. Then $p_k \mathbb \R^n
    \rightarrow K$ is the nearest points map. (This is defined using the last
    lemma.)

    If $x\in K$, then $p_k(x) = x$. $p_K$ is surjective. Usually it is not
    injective, if $K = \R^n$, then it is injective. 
** Properties of nearest point map
*** Definition (supporting hyperplane)
    A hyperplane $H$ is a supporting hyperplane if $a$ closed convex set in $\R^n$ if 
    $$H \cap K \neq \emptyset \textup{ and } K \subset H^- \textup{ or } K \subset H^+$$

    If we take a $u \in S(\R^n), \alpha \in \R^n$, $H^+ = \{x\in\R^n \vert \langle x, u \rangle \ge \alpha\}$
    $H^- = \{x\in\R^n \vert \langle x, u \rangle \le \alpha\}$.

    A picture that I didn't draw 

    Notions:
    1. Supporting half space
    2. Outer normal
    3. Inner normal
*** Aim
    We want to prove: Given a convex body and take a point in the boundary. I
    want to prove that there is a supporting hyperplane (?)
*** Lemma
    Let $\varphi \neq K \subset \R^n$ be a closed convex set. If $x \in \R^n\setminus K$,
    then the hyperplane $H = \{y \in \R^n \vert \langle y, u \rangle = 1\}$ is a
    supporting hyperplane of $K$ at $x' = p_k(x)$ where $u=\frac{x-x'}{\langle
    x', x - x'\rangle}$.

    A diagram I didn't draw (A convex body, x is a point outside, $x'$ is the
    closest element, meaning that $x'$ is on the hyperplane and we have a
    direction vector $x - x'$, we normalize this vector. ([fn:6])) [fn:5]
*** Proof
    $H$ is a hyperplane and $x' \in H$, then $\langle x - x', x - x'\rangle \ge
    0 \implies \langle x, x-x' \rangle > \langle x', x-x'\rangle \implies \langle
    x, (x-x')/(\langle x', x-x'\rangle) \implies x\in H^+$

    Now we assume that $H$ is not a supporting hyperplane, which means that
    there is as point $y$ inside $K\cap (H+\setminus H)$. Consider the
    triangle $\Delta x x' y$. Since $x$ is perpendicular to the $yx'$, the angle
    $yx_1 x$ is actute. We kinda want to prove that there is a point on the line
    segment that would minimize the distance from $x$. The argument is similar
    to the argument for last theorem. (The perpendicular from $x$ would give a
    point on the segment $x'y$ that would be the minimum.) [fn:7]
*** Lemma
    Let $K\subset \R^n$ be a closed convex set and $x\in \R^n \setminus K$.
    For a point $y$ on the half-line emanating from $x'=p_k(x)$ and containing
    $x$ holds

    $y' = p_K(y)=p_K(x) = x'$
*** Proof 
    Let $y \in [x', x]$, assume that $y' \neq x'$. We'll try to arrive at a contradiction.

    $\Vert x - x' \Vert = \Vert x - y \Vert + \Vert y - x'\Vert \ge \Vert x -
    y\Vert + \Vert y - y'\Vert$ (The second part follows from the fact that $y'$
    is the point in $K$ that is closest to $y$.)

    We apply the inequality of triangle we get that $\Vert x - x' \Vert \ge
    \Vert x - y\Vert$. This is a contradiction.
    
    We do something similar when $x$ is an element in the line segment $[y,
    x']$. (Not exactly similar, but try to arrive at a contradiction from
    drawing some triangles and what-not.)
*** Lemma Busemann and Faller's lemma
    The function $p_K$ does not increase the distance, therefore it is Lipschitz
    with constant $1$ and is uniformly continuous. This means that $\Vert
    p_k(x) - p_K(y) \Vert \le \Vert x - y\Vert$.
*** Proof
    We assume that $x' = p_K(x) \neq y' = p_K(y)$. (We draw a diagram.)

    We kinda use principles similar to the last two theorems. I skipped writing
    the proof.
* Lecture 3 <2018-10-24 Wed>
** Review
*** Nearest point map
    The definition of the nearest point map for a convex set.

    Recall that we use completeness of Real numbers for the existence of the map. [fn:8]
*** Some properties
    The nearest point map is identity in $K$. 

    Every point $y$ on the half line emanating from $x'$ containing is in the
    fiber of $x'$ with respect to $p_K$.

    $f_K$ is a Lipschitz function with constant $1$ and is hence continuous. 

    Supporting hyperplane $H \colon H \cap K = \emptyset$, $K\subset H^{-}$. 
*** Lemma
    If $x \in \R^n - K$, then $H=\{y \colon \langle y, x-x'\rangle = \langle x',
    x-x'\rangle \}$ is a supporting hyperplane of $K$ at $x'$.

    The lemma says that at every point outside of $K$, we can find a supporting
    hyperplane. What we need to prove is that at every point on the boundary we
    can find a supporting hyperplane. 
** Theorem 
   Let $K\subset \R^n$ (here $K$ is not equal to $\R^n$ be closed convex set.
   Then $K$ is equal to the intersection of all its supporting half-spaces. 
*** Proof
    Because $K$ is not $\R^n$, we have a point in the difference. Then there is
    at least one supporting hyperplane, and therefore a supporting half space.
    Let $K'$ be the intersection of all of it's supporting hyperplanes of $K$.
    It is clear that $K$ is a subset of $K'$. To prove the inclusion from the
    other side:

    Let $k'$ be an element in $K'$. Then there exists a supporting hyperplane
    $H$ at $x'=f_K(x)$ such that $K \subset H^{-}$ and $x \in inf H^{+}$. Thus
    $H$ separates $H$ and $K$, and more importantly, $x$ is not an element of
    $K'$.[fn:9]
** Theorem
   Let $K\subset \R^n$ a closed convex set and $x\in \partial K$. Then there
   exists a supporting hyperplane for $K$ containing $x$. 
*** Proof
    We define the boundary of $K$ first. Let $x\in \partial K \iff (\forall U
    \in x \textup{ and open }) U \cap K \neq \emptyset$ and $U\cap K^{c} \neq
    \emptyset$ and $x_0 \in K$.

    If $x_0$ is a point in the boundary of $K$, then there is a sequence $y_n
    \in \R^n$ such that $x_0$ is the limit of $y_n$.

    For every point $x_n = f_K(y_n)$, there is a supporting hyperplane $H_n$ at
    $x_n$. Let $s_n$ be a sequence of half lines emanating from $x_n$
    perpendicular to $H_n$. Let $S$ be a sphere with center at $x_0 \in H$ of
    small radius. Then this half line will intersect $S$ at one point. Notice
    that $y_n$ is also an element of $S_n$, then 

    $x_0 = \lim f_K(y_n')$ and $y'_{k_n}$ subsequence of $y_n'$ converging in
    $S, y_{k_n}' \rightarrow y_0 \in S$ and $x_0 = \lim f_k(y_n') = \lim
    f_K(y_{k_n}')$ and $y_0 = lim y_k' \implies f_k(y_0) = \lim f_k(y_{k_n}')$
    and $x_0 = f_k(y_0)$ and $y_0 \neq x_0$. [fn:10]
** Faces and Normal Cone

*** Definition
    Let $K\subset \R^n$ be a closed convex set. A face $F$ of $K$ is a subset of
    $K$ is a subset of the form $F = K \cap H$ where $H$ is some supporting
    hyperplane of $K$. 

    Such a face is called a proper face while $\phi$ and $K$ are also faces but
    called non-proper. (A diagram with $\emptyset$ and $K$.)

    Examples: Triangles (here faces are the edges.) For a disc, then the faces
    are points on the boundary. For a cube, the faces are the faces of the cube.
*** Lemma about convexity of face
    Every proper face of $K$ is a closed convex set.
*** Dimension
    If $F$ is a face of $K$ and $m=\dim F$, (Let affine hull of $K$ is $\R^n$.)
    1. then $m=0$ we call $F$ a vertex of $K$
    2. If $m=1$ we call $F$ an edge of $K$
    3. If $m=n-1$ we call $F$ a *facet* of $K$
    4. If $m=n-2$ we call $F$ a *ridge* of $K$.
** Lemma
   Let $F_0$ be subset of $F_1$ faces of $K$, then $F_0$ is a face of $F_1$.
*** Proof
    $F_0$ is a face of $K \implies$, therefore $F_0 = K \cap H$, where $H$ is a
    supporting hyperplane for $K$. $H$ supporting hyperplane for $K$ and
    therefore for $F_1$.

    $F_1 \cap H \subset K \cap H \subset F_1 \cap H$

    $F_0 = F_1 \cap H$. 
*** Remark
    The converse of the lemma does not hold. $F_2$ is a face of $K$ and $F_0$ is
    a face of $F_2$ implies $F_2$ is a face of $K$. The last statement is not
    true. Notice that in the above proof we need both of them to be faces of
    $A$.

    The picture: A square with a half-disc glued to the right. $F_0$ be a vertex
    on the right side and $F_1$ be the edge of the square containing $F_0$. [fn:11]
** Lemma
   Let $F_1, \cdots, F_k$ be faces of $K$, then $F=F_1\cap \cdots \cap F_k$ if a
   face of $K$.
*** Proof
    $F_i = K \cap H_i$, where $H_i = \{y \vert \langle y, u_i \rangle = 0\}$.
    ($K \subset H_i^{-1}$ [fn:12]) We can assume that $0 \in F$ which is the
    intersection of all of them $0 \in F = F_1 \cap \cdots F_k$.

    $u=u_1+\cdots+u_k$ (we can assume without loss of generality that $u \neq
    0$; this can be attained by scaling one or more $u_i$.)

    $H=\{y\vert \langle y, u\rangle = 0\}$. will be a supporting hyperplane for
    $K$ and $F = K \cap H$. $K\subset H^-$, $y\in K$, $\langle u, u\rangle =
    \langle y, u_1\rangle + \cdots + \langle y, u_k\rangle \le 0 + \cdots + 0$. [fn:13]

    The last statement implies that $y$

    $F = K \cap H$.

    $y\in F = F_1 \cap \cdots \cap F_k = (K\cap H_1) \cap \cdots \cap (K\cap H_k)$

    $\langle y, u_1 \rangle = 0, \cdots, \langle y, u_k\rangle = 0$.

    $\langle y, u_1 + \cdots + u_k \rangle = 0$.

    $\langle y, u \rangle = 0 \implies y \in H$. 

    $y \in F \cap H \subset K \subset H$. 

    $y \in K \cap H \implies y \in K$ and $y\in H$. 

    $\langle y, u_i \rangle \le 0$ 

    $\langle y, u \rangle = 0 = \langle y, u_1 \rangle + \cdots + \langle y,
    u_k\rangle$. [fn:14]
** Lemma
   - Let $F$ be a face of a closed convex set $K$ and $x, \tilde x$ be an element
   of the relative interior of $F$. Then any supporting hyperplane of $K$
   containing $x$ must contain $\tilde x$.
   - If $F, F'$ are faces of $K$ and $\relint F \cap \relint F \neq \emptyset$,
     ten $F = F'$.
*** Proof
    $H$ supporting for $F$. I didn't write this. [fn:15]
* Lecture 4 <2018-10-30 Tue>
** Review
   He did a review of stuff. 

   1. $K$ closed convex set and $H$ is a supporting hyperplane of $K$. Meaning
      that $F = K \cap H$ is a face. $\phi_1, K$ (improper) face. 
   2. $F$ face of $K$ $\implies F$ closed and convex.
   3. $F_1 \subset F_1$ faces of $K \implies F_0$ a face of $F_1$. Whereas the
      converse of the statement is not true. [fn:16]
** Definition (Normal cone)
   Let $K\subset \R^n$ be a closed convex set and $x\in K$. The *Normal cone* at
   $x$ is the set at $x$ is the set $$N(x) = -x + p_K^{-1}(\{x\})$$

   The normal cone at $x$ always contains $0$. We'll draw some examples.

   1. *A closed convex interval in $\R$*. Take a point $x$ inside the interval.
      Then $N(x) = 0$. This is because the set of all points such that the
      closest point is $x$ is just $x$.
   2. If we go at the boundary of the convex set, then the set of points that
      are closest to the point is the point and the whole half line containing
      the point. Now $N(x)$ is $[0, \infty)$ after translation. We can make a
      similar argument for the point on the other side of the boundary.
   3. *An interval in the plane*: let's say $[1, 3]$ inside $\R^2$. Now, for
      $2$, there is a perpendicular line that is closest to $2$. Now, if we
      translate it, we get a line perpendicular to $0$. Whereas, for $3$ and
      $0$, they would be two dimensional spaces (half spaces.) We can get one
      form another by doing orthogonal complement.
   4. *A triangle inside plane*. All the points inside would give us $0$.
      Whereas, for a point on one of the edge (other than vertex), $N(x)$ would
      be a line perpendicular to the edge. For an edge, it would be a
      two-dimensional space.
   5. *Remark*: Notice that for all these examples, we were able to partition
      the entire space using $N(x)$. (I think the partition thing we are talking
      about is about $p^{-1}_K$. $N(x)$ would always contain $0$.
** Lemma
   $N(x)$ is a closed convex cone. It consists of $0$ and all outer normals of
   $K$ in $x$. If $x \in \inte K$, then $N(x) = \{0\}$. 
*** Proof
    $\lambda \ge 0, u \in N(x) \implies \lambda x \in N(x)$

    $u, v \in N(x) \implies u + v \in N(x)$

    Without loss of generality, we can assume that $x=0$. 

    $u\in N(0) \implies u \in p^{-1}_K$ and a lemma gives us that $\lambda v\in
    p^{-1}_K(0)$ implies that $\lambda u \in N(0)$.

    $u, v \in N(0) \implies 0 = p_K(u) = p_K(v)$, $H_u = \{x\vert \langle u, x
    \rangle = 0\}$

    $K\subset H_u^{-1}$. Supporting hyperplane at $0$ of $K$. $H_u = \{x \vert
    \langle v, x\rangle = 0\}$, and $K\subset K_v^{-1}$.

    $x \in K, \langle u + v, x\rangle = \langle u, x\rangle + \langle v,
    x\rangle \le 0 + 0 \le 0$.

    $x \in H^{-1} \implies K \subset H^{-1}$.

    $H$ is a supporting hyperplane, then $p_{K}(u+v) = 0$.

    What we proved is that, if we take a point, the positive multiple is inside.
    We also proved that if there are two points inside, then the sum of them is
    also inside. So it's like a cone. What about closed?

    $N(x) = -x + p^{-1}(\{x\}$. Now, because $\{x\}$, is closed and $p_K$ is
    continuous, then inverse image is closed. Because the translation is an
    isometry, we are done.
** Definition (Dual cone)
   Let $G$ be a cone, then $\sigma = \{u \vert \langle \sigma, u\rangle \ge 0 \}$
   is called the *dual cone*. 
** Lemma
   If $\sigma$ is a cone with appex $0$, then $N_\sigma(0) = -\sigma$.[fn:17]
** Lemma
   Let $F$ be a face of a closed convex set of $K$ and $x, \tilde{x} \in \relint
   F$, then $N(x) = N(\tilde x)$.
*** TODO Proof
    The idea is that if there are two points in the relative interior of a face,
    then the supporting hyperplane for these points are the same. We look at all
    the normals at $x$ and $\tilde x$. [fn:18]
** A random story
   $P \rightarrow \{F \colon F \textup{ a face of P}$, for every face, we can
   talk about $N(F)$ instead of a point in the relative interior. These two
   sets, we put inclusion as a relation, these are anti-isomorphic [fn:20] These
   have some group structure and later can be used to construct affine Toric
   variety. [fn:19]
** Definition
   If $F$ is a face of a closed convex set $K$ and $x\in \relint F$, then $N(x)
   is denoted by $N(F)$ and is called the cone of normals of $K$ in $F$. [fn:21]
** Theorem
   Let $K$ be a convex body in $\R^n$ and $x(F)$ are of the relative interior
   points in $F \neq \emptyset$ or $K$. Then $\{\relint N(x(F)) \vert F \text{
   face of } K\} = \{ \relint N(F) \vert F\textup{ face of } K \}$ is a
   partition of $\R^n$.
*** TODO Proof
    Since $K$ is bounded, there exists $\alpha$ non-negative, such that $K$ is a
    subset of $H^{-1}(u, \alpha)$ where $$H(u, \alpha) = \{x \vert \langle x, u
    \rangle =\alpha\}$$

    Let's take the intersection $\cap_{K \subset H^{-1}}(u, \alpha) H^{-1}
    H^{-1}(u, \alpha)$

    There was a nice diagram. 
*** Random stuff
    $\forall u \in \R^n - \{0\}$, there exists a face $F$ of $K$, $u\in N(F)$
    and $0 \in N(K)$.

    $x \in \relint F$, and $u$ is an outer normal of $x$, then $u\in \relint
    N(F)$.[fn:22]

    He did an example with tetrahedra.

    $u \in \relint N(F_1) \cap \relint N(F_2)$, $u\ in \relint x(F_1) \cap
    \relint x(F_2)$. This means that if we take $u$ and add a point $x(F_1)$,
    $p_K(u + x(F_1)) = x(F_1)$. This means that $p_K(u + x(F_2)) = x(F_2)$

    (I missed parts of this argument.) We used boundedness of the convex body.
    If it is unbounded, the family of normal cones do not cover.
** Definition (Normal fan)
   The family $N(F)$ of $K$ is called the normal fan of $K$
** Support and distance functionn
*** Definition (support function)
    Let $K$ be a non-empty convex body. The function $h_K$ that maps $\R^n
    \rightarrow \R$, $h_K(x) = \sup_{x\in K} \langle u, x\rangle$ is the
    *support function* of $K$.[fn:23]

    We can also say it is the supremum over a fixed $x_0$.

    There was a diagram
*** Question?
    Given a ball, what is the normal fan of the space?

    If we take an interior point, then we have $0$. So we should go to the
    boundary. But each point has a supporting hyperplane. Which means, that the
    normal fan is all the half lines. The normal fan is a sphere. The normal fan
    can be ugly when we have smooth convex body. But for polytopes, it's much
    nicer.
* Lecture 5 <2018-11-06 Tue>
** Support and distance function
*** Definition
    Let $K \subset \R^n$ be a convex body. The support function of $K$ is
    $h_K\colon \R^n \rightarrow \R$, $h_K(u)= \sup_{x\in K} \langle u,
    x\rangle$
*** Lemma
    $h_{K+a}(u) = h_K(u) + \langle u, a \rangle$
*** Proof
    $K+a = \{x + a\vert x \in K\}$
    
    $h_{K+a}(u) = \sup_{x \in K} \langle u, x+a\rangle \sup_{x\in K} \langle u x \rangle + \langle u, a\rangle$

    $h_[p, q](u) = ?$, where $[p, q]\in \R^1$.
*** Lemma
    Let $K\subset \R^n$ be a convex body and $u+0$, $x_0 \in \partial K$, there
    is a supporting hyperplane at $x_0$, $H_{x_0} = \{x \vert \langle x,
    u\rangle = \langle x_0, u\rangle \}$ where $u\in N(x_0)$
    #+BEGIN_SRC artist
           ----------------------------X---------------------------
                                      / \
                                    -/ x \
                                   /      \        
                                  /        \       
                                 /          \-     
                               -/             \    
                              /                \   
                             /                  \  
                           -/                    \ 
                          /                       \
                         /                         \
    #+END_SRC
    1. The hyperplane $H_{u} = \{\langle x, u \rangle = h_K(u)\}$
    2. Every support hyperplane of $K$ has the above form.
*** Proof
    $h_K(u) = \sup\langle x, u\rangle = \langle x_0, u\rangle$ for some $x_0 \in
    K$ (because of compactness [fn:24])

    $x_0 \in H_{u} \cap K$. Let $y\in K$, then $\langle u, u \rangle \le \sup
    \langle x, u \rangle= h_K(u)$

    $y \in H^{-1}(u)$, thus $K \subset H^{-1}$.

    $H$ is a supporting hyperplane. Then it cuts $K$, $x_0 \in K \cap H$, $H
    =\{x \vert \langle x, u\rangle = \langle x_0, u \rangle \}$ where $u \in
    N(x_0)$. $N \subset H^{-1}$
** Convex function
   Let $f \colon \R^n \rightarrow \R$, we say that $f$ is convex if for every
   $x, y \in \R^n$ and $0 \le \lambda \le 1$ holds that $f(\lambda x +
   (1-\lambda)y) \le \lambda f(x) + (1-\lambda)f(y)$. If $L \subset \R^n$ is an
   affine subspace, then $f_L$ is also convex.
*** Definition
    The function $f\colon \R^n \rightarrow \R$ is *positively homogeneous* if
    for every $x\in \R^n$ and $\lambda \ge 0$ the following holds: $f(\lambda x)
    = \lambda f(x)$
** Lemma
   Let $f$ be positively homogeneous. Then $f$ is convex if and only if for all
   $x, y, \in \R^n$ the following holds:

   $f(x + y) \le f(x) + f(y)$
*** TODO Proof
    Let $f$ be convex
    $f(x)/2 + f(y)/2 \le f((x+y)/2) \le $

    $f$ satisfies the property, to show that $f$ is convex.

    $f(\lambda x + (1- \lambda)y) \le f(\lambda x) + f((1-\lambda)y) = \lambda
    f(x) + (1-\lambda) f(y)$.
** Lemma
   A function $f\colon \R^n \rightarrow \R$ is convex if and only if for all
   $x_1, \cdots, x_n$ and for all $\lambda_1, \cdots, \lambda_n$ non-negative
   such that $\lambda_0 + \cdots + \lambda_m = 1$ the following holds

   $f(\lambda_0 x_0 + \cdots \lambda_m) \le \lambda_0 f(x_0) + \cdots + \lambda_m
   f(x_m)$
*** Proof
    $\Leftarrow$, we put $x_1, \cdots, x_m = y$, it follows trivially now.

    $\implies$, induction on $m$. Assume that the condition holds with $n-1$ on
    every affine subspace of $\R^n$ of dimension $n-1$, i.e., $f(\lambda_1 x_0 +
    \cdots + \lambda_{n-1}x_{n-1}) \le \lambda_0 f(x_0) + \cdots + \lambda_{n-1}
    f(x_{n-1})$. 

    If $\lambda_0$ is zero, we are done.

    $f(\lambda_0 x_0 + \lambda_1 x_1+\cdots + \lambda_n x_m) = f(\lambda_0x_0 +
    \lambda(1-\lambda_0)(\frac{\lambda_1}{(1-\lambda_0)} + \cdots + ? x_m)) \le
    \lambda_0 f(x_0) + (1-\lambda_0) f(\frac{\lambda_1}{(1-\lambda_0)} x_1 +
    \cdots + \frac{\lambda_m}{(1-\lambda_0} f(x_m))$ We are done.
** Lemma (Convex functions are continuous)
   Every convex function $f\colon \R^n \rightarrow \R$ is continuous
*** TODO Proof
    $x_0 \in \R^n$, $T = \conv \{x_0, \cdots, x_{m+1}\}$ regular simplex, such that 

    #+BEGIN_SRC artist

                                      /|
                                     / |-\
                                    /  |  \
                                   /   |   -\
                                  /    |     \
                                 /     |      -\
                                /      |        \
                               /       |         \
                              /       -+-         -\
                             /     --/   \---       \
                            /    -/          \---    -\
                           /  --/                \---  \
                          /--/                       \---\
                         -/------------------------------\-
    #+END_SRC
    $\Vert x_0 - x_1 \Vert = \cdots = \Vert x_0 - x_{n+1}\Vert$

    $u\in U_{\partial (x_0)} = \{x \vert x - x_0\Vert < \partial \} \in T$

    $x$ belongs to one of the simplex $\{x_0, x_1, \cdots, \tilde{x_i}, \cdots,
    x_{n+1}\}$, $x\in \conv \{x_0, x_1, \cdots, x_m\}$ and $x = \lambda_0 x_1 +
    \cdots + \lambda_m x_m$ and $0\le \lambda_1, \cdots, \lambda_m < \delta <
    1$.

    $\vert f(x) - f(x_0) \vert \le \vert \lambda_0 f(x_0) + \cdots +
    \lambda_mf(x_m) - f(x_0)\vert = \vert x_1 \vert f(x_0) - f(x_0)\vert +
    \cdots + \lambda_n (f(x_n) -f(x_0)\vert$

    $f(x) > 0$ assume $\le \lambda \lambda_1 \vert f(x_1)- f(x_0) \vert +
    \cdots + \lambda_m \vert f(x_n) - f(x_0) \vert \le (\lambda_1 + \cdots
    \lambda_n) M \le n \delta M < n\delta (M+1)$

    We can call the last value as $\varepsilon$ and for given $\varepsilon > 0$,
    we can choose $\delta$. Hence the function is continuous.
** Lemma
   $f \colon \R^n \rightarrow \R$ is convex if and only if $T^{+}(f) =
   \{(x, \zeta) \vert x \in \R^n, \zeta \in \R, f(x) \le \zeta \}$ is a closed
   convex [fn:25]
*** Examples
    $y = x^2$ is a convex function.
*** TODO Proof
    $\implies f$ is convex

    $(x, \zeta), (y, \eta) \in T^{+}(f) \implies \zeta \ge f(x), \eta \ge f(y)$,
    $\lambda(x, \zeta) + (1-\lambda)(y, \eta) = (\lambda x + (1-\lambda)y,
    \lambda \zeta + (1-\lambda) \eta)$, $f(\lambda x + (1-\lambda)y) \le \lambda
    \zeta + (1-\lambda) \eta$, is an element in the inverse.

    $\Leftarrow$ $T^{+}(f)$ is convex and closed set in $\R^{n+1}$

    $f(\lambda x + \cdots + (1-\lambda)y)$

    $(x, f(x)) \in T^{+}(f)$ and similarly $(y, f(y)) \in T^{+}(f)$ these
    implies that $\lambda(x, f(x)) + (1-\lambda)(y, f(y)) \in T^{+}(f)$ this is
    equal to $(\lambda x + (1-\lambda y, \lambda f(x) + (1-\lambda) f(y)$, then
    $f(\lambda x + (1-\lambda) y) \le \lambda f(x) + (1-\lambda) f(y)$.
** Lemma
   Let $f\in \R^n \rightarrow \R$ be positively homogeneous. Then $f$ is convex
   if and only if $T^{+}(f)$ is a convex closed cone.
*** TODO Proof
    $f$ is convex.

    I didn't write this.
** Remark
   Domain should not be restricted? 
* Lecture 6 <2018-11-07 Wed>
** Support and distance function (Review)
*** Definition
    $K \subset \R^n$ non-empty convex body, we define the supoort function
    $h_K \colon \R^n \rightarrow \R$, $h_K(x) = \sup_{x\in K} \langle x, u \rangle$
*** Lemma
    $H_K(u) = \{x \in \R^n \vert \langle x , u \rangle = h_K(u)\}$ is a support
    hyperplane for $K$.
*** Definition
    $f\colon \R^n \rightarrow \R$ is convex if for all $\lambda \in [0, 1]$, and
    for all $x, y, \in \R^n$, $f(\lambda x + (1-\lambda)y) \le \lambda f(x) +
    (1-\lambda)f(y)$ positive homogeneous if $\forall \lambda \ge 0$ and
    $\forall x \in \R^n$, $f(\lambda x) = \lambda f(x)$
*** Lemma
    $f\colon \R^n \rightarrow \R$ positive homogeneous

    $f$ is convex $\iff$ $\forall x, y, \in \R^n$, $f(x+y) \le f(x) + f(y)$.
*** Lemma
    1. Every convex function is continuous
    2. $f\colon \R^n \rightarrow \R$ is convex $\iff$, $T^{-1}(f) = \{(x, \zeta)
       \vert \R^n \times \R \vert f(x) \le \zeta\}$ is convex

       $f$ is convex $\iff$ $T^{+}(f)$ is closed convex cone in $\R^{n+1}$.
** Lemma
    A support function is positive homogenous and convex.
**** Proof
     1. $h_k(\lambda u) = \sup \langle x, \lambda u\rangle = \sup_{x \in K} \lambda
        \langle x, u \rangle = \lambda \sup \langle x, u \rangle = \lambda h_K(u)$
     2. $x \in K \colon \langle x, u\rangle \le \sup_{x\in K} \langle x,
        u\rangle = h_K(u)$. $\langle x, v \rangle \le \sup_{x\in K} \langle x,
        v\rangle = h_K(v)$

        Now $\langle x, u + v\rangle \le h_K(u) + h_K(v)$, $\implies$ $\sup_{x
        \in K}\langle x, u+v\rangle \le h_K(u) + h_K(v)$.

        $h_K(u+v) \le h_K(u) + h_K(v)$, so $h_K$ is convex.
** Lemma (linearity of $h_K$)
   $h_K$ is linear on all elements of $\sum(K)$ (normal cone.)
*** Proof
    $u \in N(x_0)$, then $h_K(u)$, $H_K(u) = \{x \vert \langle x, u \rangle =
    h_k(u)\}$
    #+BEGIN_SRC artist
                                            \                 /-
                                             -\             /-
                                               -\         /-
                                                 -\     /-
                                                   -\ /-
                                                    /o-
                                                  -/   \-
                                                -/       \-
                                              -/           \-
                                            -/               \-
                                          -/                   \-
                                         /                       X
                                          --         K         /-
                                            \-                /
                                              \--           /-
                                                 \-       /-
                                                   \-    /
                                                     \-/-
                                                      /
    #+END_SRC
    So $h_K\vert_{N(x_0)}$ is the scalar product $\langle \cdot, x_0\rangle$.
** Definition
   Let $K$ be a convex body in $\R^n$ and $0$ is an element of the interior of
   $K$. (So the dimension of the convex body is $n$, i.e., the body is full
   dimensional.)

   $d_K\colon \R^n \rightarrow \R$ is defined as follows: $d_K(\lambda \bar{x})
   = \lambda$ for $\bar{x}$ element of the boundary of $K$. (When the body is
   symmetric, then one can define a norm.) Here $\lambda > 0$.
** Lemma
   Let $K$ be an n-dimensional convex body in $\R^n$, 

   1. A line $g$ intersecting the boundary of $K$ in three different points is
      contained in a support hyperplane of $K$, in particular, $G \cap \inte K$
      is empty.[fn:26]
   2. Any ray emanating from an interior point of $K$ intersects $\partial K$ at
      exactly one point.

      Here $A, B, C$ are points in the interior of the boundary and $B$ is
      inside the interior of $[A, C]$.
      
      $H$ be a supporting hyperplane through $B$, and assume that $A, C \notin
      H$, also no other point of $g$ is in $H$. Thus $H$ separates $A$ and $C$,
      contradiction.

      #+BEGIN_SRC artist
                               --
                              |  \-
                              /    \-
                             /       \-       
                            /          \-                                       /-/
                          A|             \-  C                        C     /----/
              ------------o/---------------\---------------------------o/-----g/
                          /                  \-                     /---      /
                         /                     \-               /---        -/
                        |                        \-         /---           /
                        /                          \-   /---              /
                       ---\                          ---                -/
                           ------\                                     /
                                  ------\                            -/
                                         -----\                     /
                                               ------\             /
                                                      ------\    -/
                                                             ----
      #+END_SRC
      A ray can be thought of as a $1$ dimensional half space.

      $[g_0, g_1] = g\cap K$, a convex body in $\R^1$.
** Lemma
   $d_K$ is positive homogeneous and convex.[fn:27]
*** Proof
    It is clear that the function is positive homogenous.

    *Convexity*: We are going to use one of the equivalences on convexity, i.e.,
     $f(x +y ) \le f(x) + f(y)$.

     $x = \lambda \bar{x}$ and $y = \mu \bar{y}$. If $x\in K$, then $d_K(x) \le
     1$.

     $x = \lambda \bar{x}$ and $y = \mu \bar{y}$.

     $\frac{\lambda}{\lambda + \mu}\bar{x} + \frac{\mu}{\lambda + \mu} \bar{y}
     \in K \implies$ $1 \ge d_K(\frac{\lambda}{\lambda + \mu}\bar{x} +
     \frac{\mu}{\lambda + \mu} \bar{y}) = d_K(\frac{\lambda}{\lambda +
     \mu}{x}/\mu + \frac{\mu}{\lambda + \mu} {y}/mu) =
     d_K(\frac1{\mu+\lambda}(x+y)) = \frac{1}{\lambda+\mu} d_K(x+y)$.

     $d_K(x+y) \le \lambda + \mu = d_k(x) + d_k(y)$.[fn:28]
** Definition (Centrally symmetric)
   A convex body $K$ in $\R^n$ is said to be *centrally symmetric* if there
   exists a point in $K$ such that $K = \rho(K)$ where $\rho$ is the central
   symmetry with respect to $c$.
 
   Defining $\rho$? With respect to $c$, $\rho(x) = 2c - x$.

   Examples:
   1. Point.
   2. Disc
   3. Cube
   4. Octahedron. Cross polytope? In $\R^n$, we have a standard basis $(e_1,
      \cdots, e_n)$. We take $\{e_1, \cdots, e_n\}$. The convex hull $\{e_1,
      \cdots, e_n, -e_1, \cdots, -e_n\}$ is a cross polytope. A three
      dimensional cross polytope is an Octahedron.
** Theorem
   Let $K$ be a centrally symmetric convex body with $0\in \inte K$, as its
   center of reflection. Then $d_K$ defines a norm in $\R^n$ satisfying for all
   $\lambda \in \R$ and $x, y \in \R^n$
   1. $\Vert x \Vert = 0 \iff x =0$
   2. $\Vert \lambda x \Vert = \vert \lambda \vert \Vert x \Vert$
   3. $\Vert x +y \Vert \le \Vert x \Vert + \Vert y\Vert$
** Example
   1. *Maximum norm* in $\R^2$: $\Vert (x_1, x_2)\Vert =- \max \{\vert x_1\vert,
      \vert x_2 \vert \} = d_K(x_1, x_2)$. What is the convex body that induces
      this norm? A square (with the usual orientation.)
   2. *Manhattan norm*: $\Vert(x_1, x_2)\Vert = \vert x_1 \vert + \vert x_2
      \vert$. The convex body for this one would be a square (actually a cross
      polytope; the slanted square.)
** Polar bodies
   Let $K$ be a convex body in $\R^n$ with $0 \in \inte K$. For $u \neq 0$, let

   $$H_u^{-1} = \{x \in \R^n \vert \langle x, u \rangle \le 1\} = \cap_{x\in
   \partial K} H^{-1}_u$$

   In particular, $H^{-1}_0= \R^m$. The polar body of $K$ is $K^{*} = \cap
   H^{-1}_u$.
** Theorem
   Let $K$ be a convex body with $0 \in \inte K$. Then $K^{**} = K$
*** Proof
    $K^{*} = \cap H^{-1}_u = \cap_{u \in K} \{x \vert \langle x, u \le 1 \} =
    \{x \vert \forall x \in K, \langle x, u \rangle \le 1\}$

    $K^{*} = \{x \vert \langle x, K \rangle \le 1\}$

    $K^{**} = \{y \vert \langle y, K^{*} \rangle \le 1\}$

    $y \in K \implies \forall x \in K^*, \langle x, y \rangle \le 1$

    $\langle K, y^{*} \le 1 \implies \langle K^{*}, y\rangle \le 1 \implies y
    \in K^{**}$, $K \subset K^{**}$.
* Lecture 7 <2018-11-13 Tue>
** Polar Body
   Let $K$ be a convex body and $0\in \inte K \subset \R^n$. For every $u\in
   \R^n - \{0\}$, we define $H_u^{-} = \{x \vert \langle x, u \rangle \le 1\}$

   Remember that $H_K(u) = \{x \vert \langle x, u \rangle = h_K(u)\}$ is a
   support hyperplane in the direction $u$ of the convex body.

   The polar body $K^{*}$ is the intersection $\cap H^{-1}_u$, when $u$ is
   element of $K$. This is the same as $\cap_{x\in \partial K} H_u^{-1}$. The
   idea is that for $m>1$, it is easy to see that $H^{-1}_{mu} \subset H^{-1}_u$
** Theorem
   Let $K$ be a convex body in $\R^n$ such that $0 \in \inte K$. Then $K^{**} =
   K$.
*** Proof
    We use the notation $\{x \vert \langle x, K \rangle \le 1\}$ to denote
    $K^{*}$. Similarly $\{x \vert \langle x, K^{*} \rangle \le 1 \}$ to denote
    $K^{**}$.
    1. $y\in K$, from definition of $K^{*}$, it follows that $\forall x \in
       K^{*} \vert \langle x, y \rangle \le 1 \implies \langle K^{*}, y\rangle
       \le 1/2$ implies that $y \in K^{**}$.
    2. $x \in K^{**} - K$, $x' = p_K(x)$, $u = \frac{x - x'}{\langle x', x -
       x'\rangle}$, then $H_u = \{x \vert \langle x, u \rangle \le 1 \}$, $x \in
       H^{+}_u$ (we assume that $x$ is a point in the interior.) and $K \subset
       H^{-1}_u$. $K \subset H_u^{-1} \implies \forall x \in K$, $\langle x, u
       \rangle \le 1 \implies \langle K, u \rangle \le 1$. This implies that $u
       \in K^{*}$.
       
       $x \in K^{**}$ and $u\in K^{*}$. Now $\langle x, u \rangle \le 1$, $x\in
       H^{-1}_u$ contradiction.
** Random stuff
   If $K$ is a convex body, it will turn out that the support function of the
   convex body will be the distance function of the dual body and vice-versa.
** Theorem
   Let $K \subset \R^n$ be a convex body and $0 \in \inte K$, then $d_K =
   h_{K^{*}}$ and $d_{K^{*}} = h_{K}$.
** Lemma
   Let $K_1 \subset K_2$, then $K_2^{*} \subset K_1^{*}$. The proof is not too hard.
** Lemma
   Let $K \subset \R^n$ and $0 \in \inte K$. Then $H_u$ is a support hyperplane
   for $K^{*}$ if $u \in \partial K$.
*** Proof
    We know that $K^{*} = \cap_{x\in \partial K} H^{-1}_u$. We take $0\in \inte
    K$, and in each direction $u$, we have a unique intersection with the body.
    In each direction, we have exactly one point on the boundary.

    $K^{*} = \cap H^{-1}_u$ convex body $0 \in \inte K^{*}$.

    The proof is kinda easy. The proof involved constructing a new convex body,
    $\tilde{K} = \conv \{ \beta u \vert u \in \partial K\}$. Apparently there is
    a problem with this proof. [fn:29]
** Proof of duality of distance and $h_K$
   Let $u \in \partial K \implies d_K(u) = 1$

   But we just argued that on the boundary, it is a support hyperplane. Thus
   $H_u = H_K(u) = \{x \vert \langle x, u \rangle = h_{K^{*}}(u)\}$ and $\{x \vert
   \langle x, u \rangle = 1\}$. Thus $h_{K^{*}}(u) = 1$.
** Theorem
   Let $K \subset \R^n$ be a convex body with $0 \in \inte K$.

   $K^{+} = \Gamma^{+}(d_K) \subset \R^{n+1}$ and $H = \{(x, 1) \vert x \in
   \R^{n}\} \subset \R^{n+1}$. Then
   1. $\partial K_{+}$ is the graph of $d_K$ in $\R^{n+1}$.
   2. $K_{+} \cap H$is a translation of $K$.
   3. $K^{*}_+ \cap H$ is a translate of $K^{*}$
   4. $K_{+}, K^{*}_+$ are convex with appex $O$ in $\R^n$.
** Theorem
   Every positive homogenous and convex function $h \colon \R^n \rightarrow \R$
   is a support function $h = h_K$ of a unique convex body $K$ whose dimension
   possibly $<n$. [fn:30]
** Radon's theorem
   Let $X$ be a set of points in $\R^n$, and $\vert X \vert \ge n+2$, then there
   is a partition of $X$ into $P$ and $N$, such that a convex hull of $P$ and
   $N$ intersect.
*** Proof
    One can assume that $\vert X \vert = n+2$ and $X = \{x_1, \cdots, x_{n+2}$.
    There is an affine dependence $\lambda_1 x_1 + \cdots + \lambda n+2 x_{n+2}
    = 0$ and $\lambda_1 + \cdots + \lambda_n = 0$ and not all $\lambda_i$ are
    zero.

    So we can write it in terms of $\sum_{i \in P} \lambda_i x_i = \sum_{i \in
    N} -\lambda_i x_i$.

    Now $A = \sum_{x_i \in P} \lambda_i = \sum_{x_i \in N} \lambda)i > 0$.

    Now it is pretty easy to see that there is a point in intersection.
*** Questions?
    Why not two? How many points should we have to say something like, we can
    partition into $100$ sets, but their convex hulls intersect.
** Affine space
   $(\mathscr{E}, E, \theta \colon \mathscr{E} \times \mathscr{E} \rightarrow
   E)$
   
   1. For all $A \in \mathscr{E}$, $\theta_A \colon \mathscr{E} \rightarrow E$
      is a bijection. $B \mapsto \theta(A, B)$.
   2. For all $A, B, C \in \mathscr{E}$, $\theta(A, B) + \theta(B, C) =
      \theta(A, C)$.
      
   Example: $(E,E, \theta(u, v) = v - u)$ This is an example with $E = \R^n$
   that we work with.
** Random stuff
   A map $(\mathscr{E}, E, \theta) \rightarrow (\mathscr{F}, F, \theta)$.
   $\theta(\varphi, f), \varphi \colon \mathscr{E}\rightarrow \mathscr{F}$.

   And $f\colon E \rightarrow F$, and for all $A, B \in \mathscr{E}$ and
   $\theta(\varphi(A), \varphi(B)) = f(\theta(A, B))$.
** Radon's theorem rephrased
   Let $T_n+1$ be a simplex and $a \colon T_{n+1}\rightarrow \R^n$ be an affine
   map. Then there exists faces $\sigma$ and $\tau$ of the simplex such that
   their $a$ images in $\R^n$ would intersect.

   Why is this the same?

   Why affine map? If we have an affine map, he the image of the simplex is the
   convex hull of all points on vertices.

   What about continuous map?
** Continuous variant of Radon's theorem (Topological Radon)
   The continuous invariant is also true.
** Helly's theorem
   Let $K_1, \cdots, K_n$ be a collection of convex sets in $\R^d$ such that
   every subcollection of $d+1$ of them intersects $\neq \emptyset$, then the
   complete family intersects.
*** Proof
    Induction on $n$. If $n \le d+1$, there is nothing to prove. Let $n \ge
    d+2$, assume that it holds for $n-1$, consider the following points $x_i \in
    \cap_{1 \le j \le n, j \neq i} K_j$. By induction hypothesis, this has to
    intersect. In this way, have points $x_1, \cdots, x_n$. By assumption, $n\ge
    d+2$, we can apply Radon's theorem. And the point in the Radon's theorem
    belongs to the intersection of everything.
** Helly's theorem*
   Let $\{K_i \vert i \in I\}$ be a family of *convex bodies* in $\R^n$ such
   that every family of $d+1$ of them intersects, then the whole intersection is
   non-empty. The difference between these and the before theorem is that the
   index set may be infinite. This follows from the above theorem because of
   compactness. (Use the closed set intersection of compactness; the
   finite-intersection property.)
* Lecture 8 <2018-11-14 Wed>
** Helly's theorem
   Let $K_1, \cdots, K_n$ be a collection of convex sets in $\R^d$ such that
   every subcollection of $d+1$ of them intersects $\neq \emptyset$, then the
   complete family intersects.
*** Is convexity important? 
    If we drop convexity of one set, we would not have such a result. Convexity
    is an important assumption. It's not too hard to construct a
    counter-example.
*** Remark about infinite families
    We can replace the finite family with an infinite family, but of convex
    bodies.
** Charatheodery's theorem
   If $x\in \conv X$, then there exists a collection of at most $d+1$ points in
   $X'$ of $X$ such that $x\in \conv X' \subset \conv X$.
** Colorful Charatheodery's theorem (theorem by Inere Bardney)
   Let $S_1, \cdots, S_{d+1}$ be collections of points in $\R^d$ and $x \in
   \conv S_1 \cap \conv S_2 \cap \cdots \cap S_{d+1}$. Then there are points
   $x_1 \in S_1, x_2 \in S_2, \cdots, x_{d+1} \in S_{d+1}$ such that $x$ is an
   element of $\conv \{x_1, \cdots, x_{d+1}\}$.[fn:31]
*** Proof
    A proof using infinite descent. 

    Without loss of generality, we can assume that each of $S_i$ are finite.
    Without loss of generality, we can assume that $x = 0$, $d(x, \conv\{x_1,
    \cdots, x_{d+1}\})$ for any choice $x_1 \in S_1, \cdots, x_{d+1} \in
    S_{d+1}\}$. This is a finite set.Let $d$ be the minimum distance. If $d=0$,
    we are done. Since we have a set of points $x_i \in S_i$, such that the
    distance of the point $x$ and the convex hulls formed by the points is zero,
    which means $x$ is in the convex hull and we are done.

    Assume that $d \neq 0$, then one of $d$ minimizes, then there exit
    $\bar{x_1}\in S_1, \cdots, \bar{x}_{d+1} \in S_{d+1}$. But then, since the
    convex hull is closed, the distance is attained at a point. There is a point
    $z$ such that $d(x, z) = d$.

    Let $H$ be the hyperplane perpendicular or orthogonal to the vector $z-x$,
    oriented such that $x \in H^{-1}$.

    We prove first that $S = \conv\{\bar{x}_1, \cdots, \bar{x}_{d+1}\} \subset
    H^{+}$ Assume that $y \in S$ is in the interior of $H^{-1}$. Consider the
    triangle $\Delta xyz$.We know that the angle $xzy < \frac{\pi}{2}$. Second
    thing we know is that the edge $zy$ \subset S$. $xz \le xy$, because $xz$ is
    the minimum distance.

    Consider a point $t \in (z, y) \subset S$ such that the angle $\angle xtz >
    \angle xzt$. But then $\vert xz \vert > \vert xt\vert$. This is a
    contradiction, since $t$ is in $S$ and $xt$ minimizes the distance of $x$ to
    $S$.

    Thus there is no point of $S$ in the interior of $H^{-}$.

    $z \in \conv\{\bar{x}_1, \cdots, \bar{x}_{d+1}\} \cap H$. We can assume that
    all of them are in $H$, if not we can put zero. (Why?)

    $z \in \cone\{\bar{x}_1, \cdots, \hat{x}_i, \cdots x_{d+1}\}$

    Since $x\in \conv S_j$, there is $\tilde{x_j} \in S_j \cap H$ 

    $z \in \conv\{\bar{x}_1, \cdots, \tilde{x_j}, \cdots, \bar{x}_{d+1}\}$. We
    call the last set $\bar{S}$.

    Consider the triangle $\Delta xz\bar{x}_j$ and we know.
    1. $[z, \bar{x}_j] \subset \bar{S}$
    2. $\angle \tilde{x_j}zx < \frac{\pi}{2}$.
    3. $\vert xz\vert < \bar x \tilde{x}_j\vert$.

    This is similar to the last part, we have a contradiction. Taking a point $t
    \in xz$ close to $z$ such that $\angle tzx < \angle ztx$ we get that $\vert
    xt \vert < \vert xz\vert = d$. This is a contradiction since $t \in \bar{S}
    = \conv\{\bar{x}_1, \cdots, \bar{x}_{d+1}\}$. We are done.
** Tverberg's theorem
   [[https://en.wikipedia.org/wiki/Tverberg%27s_theorem][Wikipedia]] Let $N=(d+1)(r-1)$ and $X \subset \R^d$ with $\vert X \vert \ge
   N+1$. Then there exists a partition $X_1, \cdots, X_r$ of $X$ such that

   $$\conv X_1 \cap \conv X_2 \cdots \cap \conv X_r \neq \emptyset$$

   Can we drop one point and still get the result? No, the number of points is
   minimal. I didn't write the counter example.

   A simplex. Consider a simplex $\Delta_N = \conv \{x_0, \cdots, x_N\} \mapsto
   \R^n$ (this is an affine map)

   $X = \vert_{i=0}^n \lambda_i x_i \mapsto \sum_{i=0}^N \lambda_i f(x_i)$.

   The affine map sends a face of the simplex to the convex hull of the
   simplex.
*** Question (Topological Tverberg Conjecture)
    For all $d \ge 1$, $r \ge 2$ and $n=(d+1)(r-1)$ and any continuous map
    $f\colon \Delta_N \rightarrow \R^d$, there exits $G_1, \cdots, G_r$ pairwise
    disjoint faces of $\Delta_N$ such that $f(\sigma_1) \cap \cdots \cap
    f(\sigma_r) \neq \emptyset$.

    This holds if and only if $r$ is a power of a prime. This has something to
    do with the elementary Abelian groups and their structure. Elementary
    Abelian groups are direct sums of cyclic groups $\Z_p \oplus \cdots \oplus
    \Z_p$.
** Weak Tverberg theorem
   Let $n\ge (r-1)(d+1)^2 + 1$ and $X = \{x_1, \cdots, x_n\} \subset \R^d$.
   There is a partition $I_1, \cdots, I_r$ of $[n] = \{1, 2, \cdots, n\}$ such
   that the intersection of the convex hulls 

   $$\cap_{n \le j \le r} \conv\{x_i \vert i \in I_j\} \neq \emptyset$$

   It's the same as last theorem, except we have more points.
*** Proof
    $k=(d+1)(r-1)$, $s=n-k \ge (r-1)(d+1)^2 + 1 - (r-1)(d+1) = (r-1)(d+1)d +1$

    Now $Y_1, \cdots, Y_{d+1} \subset X$ and each $\vert Y_i\vert = s$, we claim
    that the intersection is non-empty.

    Proof: $\vert Y_1 \cap Y_2\vert = \vert Y_1\vert + vert Y_2\vert - \vert Y_1
    \cup Y_2 \vert \ge n-k+n-k-n = n-2k \ge (r-1)(d+1)^2 - (r-1)\cdot 2 \cdot
    (d+1)$

    $\vert Y_1 \cap Y_2 \cap Y_3 \vert = \vert Y_1 \cap Y_2 \vert + \vert
    Y_3\vert - \vert (Y_1 \cap Y_2) \cup Y_2\vert \ge n-2k+n-k-n = n-3k$

    $\vert Y_1 \cap \cdots \cap Y_{d+1} \vert \ge n - (d+1)k \ge 1$.
*** TODO Corollary
    If $Y_1, \cdots, Y_{d+1} \subset X$ with $\vert Y_i\vert = s$, then $\conv
    Y_1 \cap \cdots \cap \conv Y_{d+1} \neq \emptyset$.

    $Y=\{\conv Y\vert Y \subset X, \vert Y\vert \ge s\}$, finite family of
    convex sets.

    Every subfamily of $d+1$ elements non-trivially intersect. Then by Helly's
    theorem there is

    $z \in \cap Y$. $z\in \conv X$. But now Charatheodery tells that $\exits x_1
    \subset X$ such that $\vert X_1 \vert = d+1$ and $z \in \conv X_1$.

    $\vert X - X_1 \vert = n - (d+1) \ge (n-1)(d+1)^2 + 1 - (d+1) \ge s$. $z \in
    \conv(X - X_2) \implies \exits x_2 \subset X - X_1), x_2 = d+1$ and $z =
    \conv X_2$. I can do this $r$ times and we are done.
** Optimal colored Tverberg theorem
*** Topological Tverberg theorem
    $d \ge 1$, $r \ge 2$, power of prime, $f \colon \Delta_N \rightarrow \R^d$,
    a continuous map. Here $N=(d+1)(r-1)$.

    Then there exists $\sigma_1, \cdots, \sigma_r$ pairwise disjoint faces of
    $\Delta_N$, $f(\sigma_1) \cap \cdots \cap f(\sigma_r) \neq \empty set$.
*** Optimal colored version
    $d \ge 1$, $r\ge 2$ is prime. $f\colon \Delta_N \rightarrow \R^d$ continuous
    such that $N=(d+1)(r-1)$ and $\vect \Delta_N = C_0 \sqcup \cdots \sqcup C_m$
    such that $\vert C_i \vert \le r- 1$ implies that there exists $\sigma_1,
    \cdots, \sigma_r$ (pairwise disjoint faces of $\Delta_N$ such that
    $f(\sigma_1)\cap \cdots \cap f(\sigma_r) \neq \emptyset$.

    For all $i, j$, $\vert \sigma_i \cap \sigma_j \vert \le 1$. About
    Barany-Larwan conjecture for primes - 1.
* Footnotes

[fn:31] Think about $S_i$ as having a unique color. I guess each $S_i$s are
disjoint.

[fn:30] We didn't prove this and the theorem before.

[fn:29] Apparently it's slightly different in the book. Which book?

[fn:28] We now have the triangle inequality. But for the norm, we should also
have the following property: $\Vert \lambda x \Vert = \vert \lambda \vert \Vert
x \Vert$.

[fn:27] Positive homogenous is a function $f(\alpha x) = \alpha^k f(x)$.

[fn:26] If we put $\relint$ instead of $\inte$, it will not work. This is why we
use the assumption, that we are using a full dimensional convex body rather than
a random one.

[fn:25] $f(x) = 1/x$ and $f'(x) = - 1/x^2$, but the function is not always
increasing (but on connected components.)

[fn:24] There is also the notion of sequential compactness. Compactness and
sequential compactness are not equivalent.

[fn:23] We use the fact that every compact function has a supremum.

[fn:22] This may or may not be true. Wasn't discussed in the class.

[fn:21] Connection with analysis: For a smooth real valued function from $\R$,
we have a unique normal, whereas for a non-smooth point, there are several
different normals (or supporting hyperplane.) We'll do something similar in our
course.

[fn:20] I think it means, the inclusion becomes opposite in the other space.

[fn:19] The construction is similar to the construction of a toric variety.

[fn:18] I think I'm missing some stuff.

[fn:17] He didn't prove this, but it's obvious.

[fn:16] Remember the example with a square and a half disc glued to the right side of the square?

[fn:15] This means that we can choose a supporting hyperplane by choosing a
point inside relative interior.

[fn:14] Question: what about infinite intersection. 

[fn:13] This is why we assumed that $0$ is in $K$, otherwise we'll have to play
around.

[fn:12] We need the next statement for making this assertion. 

[fn:11] Apparently the statement would be true for polytopes, i.e., the converse
holds for polytopes. This is one of the reason we're interested in polytopes. 

[fn:10] I don't understand what's happening at the end 

[fn:9] We're interested in spaces that can be formed by finitely many
intersections of hyperplanes. These will be called Polyhedra. An non-example is
a disc.

[fn:8] Did I write this statement correctly?

[fn:7] I think I made mistakes in framing at the beginning of the paragraph.

[fn:6] I think I made a mistake in what I wrote here.

[fn:5] If $p_K$ is what we already know, we know that every point in $p_k(\R^n \
K)$ \subset M$ has a supporting hyperplane. We'll try to figure out more about
this set.

[fn:4] Interestingly, convexity of the set is not used here. It's probably only
needed for the uniqueness.

[fn:3] Solve the exercise for Helly's theorem. 

[fn:2] What if $M$ is a disc?

[fn:1] Wikipedia article about Radon'n theorem says that this is true. Crazy.
